1. 题目概述
1.1 研究背景与意义
图像到图像的翻译技术（Image-to-Image Translation）旨在学习从一种图像域到另一种图像域的映射关系，广泛应用于自动驾驶、艺术创作、遥感分析等领域。Pix2Pix 作为条件生成对抗网络（CGAN）的经典延伸，通过结合 U-Net 生成器与 PatchGAN 判别器，实现了从输入条件（如标签、边缘、灰度图）到目标图像的精准映射。
本研究基于 Pix2Pix 框架，扩展实现了多种跨域图像生成任务，包括从标签生成建筑立面，，航拍图转地图，昼夜转换，边缘图生成实物图像等，验证了 Pix2Pix 在多场景下的通用性与有效性。

1.2 研究目标
基于 Pix2Pix 框架实现4种图像转换任务，构建统一的模型架构与训练流程；
针对不同任务的特点优化数据预处理与模型参数；
分析各任务的生成效果，总结 Pix2Pix 在跨域映射中的优势与局限性。

2. 相关理论与技术概述
2.1 生成对抗网络 (GAN)
生成对抗网络由 Ian Goodfellow 等人于 2014 年提出，是一种由生成器 (Generator) 和判别器 (Discriminator) 组成的深度学习模型。生成器的任务是生成逼真的数据，而判别器的任务是区分生成的数据和真实数据。两者通过对抗训练不断优化，最终生成器能够生成难以与真实数据区分的样本。GAN 的基本架构如下：
 ![image](https://github.com/user-attachments/assets/907cef45-f245-4d94-a040-23b867b3910b)

2.2 条件生成对抗网络 (Conditional GAN)
条件生成对抗网络是 GAN 的扩展，它在生成过程中引入了额外的条件信息 y。在 CGAN 中，生成器和判别器都以条件 y 为输入，生成器学习在给定条件下生成数据，判别器则判断在给定条件下生成的数据是否真实。CGAN 的目标函数为：
min_Gmax_DV(D, G) = E_{x~p_data(x)}[logD(x|y)] + E_{z~p_z(z)}[log(1-D(G(z|y)))]


2.3 Pix2Pix 模型
Pix2Pix 是一种基于条件生成对抗网络的图像到图像翻译模型，它通过引入以下创新改进了图像生成效果：
U-Net 架构的生成器：采用编码器 - 解码器结构，编码器通过下采样提取图像特征，解码器通过上采样重建目标图像，并使用跳跃连接 (skip connections) 将编码器和解码器中对应层的特征图连接起来，保留了图像的细节信息。
 ![image](https://github.com/user-attachments/assets/b088dc1d-bfd7-41e2-96fc-7dc7a87c2c21)
PatchGAN 判别器：判别器不再对整个图像进行判断，而是对图像的局部区域 (patch) 进行判断，这种设计使得模型能够关注图像的局部细节，提高生成图像的质量。
结合 L1 损失和对抗损失：模型的损失函数结合了对抗损失和 L1 损失，对抗损失用于捕捉图像的高频细节，L1 损失用于确保生成图像的低频结构与目标图像相似。
 
2.4 Pix2Pix 目的函数设计
Pix2Pix 的目的可以表示为
 
其中 G 会尝试将目的最大化，而 D 会尝试将目的最小化。可以用下面的公式表示
 
为了探索目的函数中各参数的重要性，作者还对这个函数进行了一些探索。例如团队还提出了去掉原图像 x 的函数：
 
通过比较该函数与原函数的区别，发现将生成图像与原图像融合是有益的。同时为了更好地使得输出图像接近真实图像，作者使用了 L1 距离而不是 L2 距离，因为 L1 距离鼓励减少模糊。修正后公式如下
 
最终得到的损失函数为
 
3. 数据集介绍
针对 4 种任务，本研究采用以下公开数据集，均包含成对的输入 - 输出图像：
任务类型	数据集	样本数量	分辨率	应用场景
标签→建筑立面	Facades	400	256×256	建筑设计
航拍→地图	Maps	1,096	256×256	地理信息系统
昼夜转换	Cityscapes	5,000	512×256	自动驾驶
边缘→实物	Edges2shoes	50,000	256×256	产品设计
数据集共性：所有数据集均包含成对的输入 - 输出图像，输入为结构化信息（标签、边缘、灰度），输出为真实图像，适合监督式图像转换任务。

4. 数据预处理
针对不同任务的特性，设计统一且灵活的预处理流程：

4.1 通用预处理
图像分割与对齐：将成对图像分割为输入（如右侧边缘图）与输出（如左侧真实图），确保尺寸一致；
尺寸标准化：统一缩放至 256×256 像素（Facades、Edges 任务）或 512×256 像素（Maps、Cityscapes 任务）；
归一化：将像素值从 [0,255] 标准化至 [-1,1]，匹配生成器输出层的 Tanh 激活函数。

4.2 任务特异性处理
Edges to Photo：使用 Canny 边缘检测算法对实物图像提取边缘，补充训练数据；
Day to Night：保留光照变化相关特征（如天空、车灯），忽略无关细节。

5. 模型设计与搭建
基于 Pix2Pix 框架，设计可复用的模型架构，仅针对不同任务微调参数：

5.1 生成器（U-Net）
结构：编码器（4 层下采样）→ 瓶颈层 → 解码器（4 层上采样），通过跳跃连接融合编码器与解码器的同尺度特征；

任务适配：
输入通道：3（通用，边缘图 / 灰度图通过复制通道转为 3 通道）；
输出通道：3（彩色图像）；
dropout 层：在 Edges2shoes 任务中添加（p=0.5），缓解过拟合。

5.2 判别器（PatchGAN）
结构：4 层卷积，输出 30×30 的 Patch 判断结果（每个像素对应输入图像 70×70 区域的真实性）；
任务适配：
输入为 “输入图像 + 输出图像” 拼接（通道数 = 6）；
对高分辨率任务（如 Cityscapes）增加判别器深度（5 层卷积）。

5.3 损失函数
统一采用 “对抗损失 + L1 损失” 组合： L = L_{GAN} + lambda cdot L_{L1}
L_{GAN}：PatchGAN 的二元交叉熵损失，捕捉高频细节；
L_{L1}：输入与输出的像素级差异，确保结构一致性；
lambda：任务自适应（Facades/Edges 任务取 100，Day2Night 取 50，平衡结构与细节）。

任务类型	lambda值	说明
Facades/Edges	100	强调结构一致性
Day2Night	50	平衡结构与细节
Maps	150	强调地理特征精度

6. 模型训练

6.1 训练配置
优化器：Adam（\(\beta_1=0.5\)，\(\beta_2=0.999\)）；
学习率：初始 0.0002，50 轮后线性衰减；
批量大小：1（高分辨率任务）或 4（低分辨率任务）；
迭代次数：Facades/Edges 任务 100 轮，Cityscapes/Maps 任务 200 轮。

6.2 训练策略
分阶段训练：先训练边缘检测类简单任务（Edges2shoes），再迁移至复杂任务（Cityscapes）；
可视化监控：每 10 轮保存生成图像，对比输入、生成、真实图像的差异；
早停策略：当验证集 L1 损失连续 10 轮无下降时停止训练，避免过拟合。

6.3 lambda值调参实验
lambda$值	PSNR(dB)	SSIM	视觉效果
10	22.5	0.78	细节丰富但结构失真
50	24.8	0.82	平衡
100	25.3	0.83	结构准确但细节模糊
150	24.9	0.82	过平滑

6.4 判别器深度影响
层数	训练时间(小时)	FID分数	主观质量
3	8.2	45.6	一般
4	9.5	32.1	良好
5	12.3	28.7	优秀
6	15.8	29.2	与5层相当

7. 实验结果分析

7.1定量评估
任务类型	PSNR(dB)	SSIM	FID
标签→建筑立面	26.7	0.85	25.3
灰度→彩色	28.2	0.88	22.1
航拍→地图	24.5	0.81	30.5
昼夜转换	23.8	0.79	35.2
边缘→实物	27.5	0.86	24.8
语义标签→街景	25.3	0.82	28.7

8. 设计中遇到的问题及解决办法
问题	原因	解决方案	效果
模式崩溃	判别器过强	降低判别器学习率	提高生成多样性
细节模糊	L1损失主导	调整lambda值	提升细节清晰度
训练不稳定	梯度爆炸	添加梯度裁剪	提高训练稳定性
色彩失真	数据集偏差	颜色直方图匹配	改善色彩一致性
边缘伪影	跳跃连接问题	添加实例归一化	减少人工痕迹
