{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'datasets/facades\\\\val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 402\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\n\u001b[0;32m    401\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/pix2pix_epoch_100.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAlignedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/facades\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m demo \u001b[38;5;241m=\u001b[39m create_interface(model_path, dataset)\n\u001b[0;32m    405\u001b[0m port \u001b[38;5;241m=\u001b[39m find_available_port()\n",
      "Cell \u001b[1;32mIn[1], line 387\u001b[0m, in \u001b[0;36mAlignedDataset.__init__\u001b[1;34m(self, root, phase)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, phase)\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'datasets/facades\\\\val'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import gradio as gr\n",
    "import functools\n",
    "import socket\n",
    "\n",
    "\n",
    "# è®¾å¤‡é…ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "\n",
    "# 1. æ¨¡å‹å®šä¹‰ï¼ˆä¸¥æ ¼æŒ‰ç…§åŸå§‹pix2pixæ¶æ„ï¼‰\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, \n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "            \n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "            \n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"ä¸¥æ ¼æŒ‰ç…§åŸå§‹pix2pixçš„U-Netç»“æ„å®ç°\"\"\"\n",
    "    def __init__(self, input_nc=3, output_nc=3, num_downs=8, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        \n",
    "        # æ„å»ºU-Netç»“æ„\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, \n",
    "                                            submodule=None, norm_layer=norm_layer, innermost=True)\n",
    "        \n",
    "        # ä¸­é—´å±‚\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, \n",
    "                                                submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        \n",
    "        # é€æ¸å‡å°‘é€šé“æ•°\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        \n",
    "        # æœ€å¤–å±‚\n",
    "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, \n",
    "                                           submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
    "        \n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "# 2. è¾…åŠ©å‡½æ•°\n",
    "def find_available_port(start_port=7868, end_port=7968):\n",
    "    for port in range(start_port, end_port + 1):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    raise OSError(f\"æ— æ³•åœ¨èŒƒå›´ {start_port}-{end_port} å†…æ‰¾åˆ°å¯ç”¨ç«¯å£\")\n",
    "\n",
    "\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"å¢å¼ºç‰ˆï¼šå¤„ç†å„ç§æ ¼å¼çš„å›¾åƒè½¬æ¢\"\"\"\n",
    "    try:\n",
    "        if isinstance(input_image, torch.Tensor):\n",
    "            # å¤„ç†PyTorchå¼ é‡\n",
    "            image_tensor = input_image.data.cpu().float().numpy()\n",
    "            \n",
    "            # è°ƒè¯•ä¿¡æ¯\n",
    "            print(f\"å¼ é‡å½¢çŠ¶: {image_tensor.shape}\")\n",
    "            \n",
    "            # ç§»é™¤batchç»´åº¦\n",
    "            if image_tensor.ndim == 4:\n",
    "                image_tensor = image_tensor[0]\n",
    "                \n",
    "            # è½¬æ¢é€šé“ç»´åº¦ï¼š(C, H, W) -> (H, W, C)\n",
    "            if image_tensor.shape[0] in [1, 3]:\n",
    "                image_tensor = np.transpose(image_tensor, (1, 2, 0))\n",
    "                \n",
    "            # å¤„ç†å•é€šé“å›¾åƒï¼šå¤åˆ¶ä¸ºä¸‰é€šé“\n",
    "            if image_tensor.shape[-1] == 1:\n",
    "                image_tensor = np.repeat(image_tensor, 3, axis=-1)\n",
    "                \n",
    "            # å½’ä¸€åŒ–åˆ°[0, 255]\n",
    "            image_tensor = (image_tensor + 1) / 2.0 * 255.0\n",
    "            image_tensor = np.clip(image_tensor, 0, 255)\n",
    "            \n",
    "            return image_tensor.astype(imtype)\n",
    "        \n",
    "        # å¤„ç†numpyæ•°ç»„\n",
    "        if isinstance(input_image, np.ndarray):\n",
    "            # è°ƒè¯•ä¿¡æ¯\n",
    "            print(f\"numpyæ•°ç»„å½¢çŠ¶: {input_image.shape}\")\n",
    "            \n",
    "            # å¤„ç†å•é€šé“å›¾åƒ\n",
    "            if input_image.ndim == 2:\n",
    "                return np.repeat(input_image[:, :, np.newaxis], 3, axis=2).astype(imtype)\n",
    "            if input_image.ndim == 3 and input_image.shape[-1] == 1:\n",
    "                return np.repeat(input_image, 3, axis=2).astype(imtype)\n",
    "            return input_image.astype(imtype)\n",
    "            \n",
    "        # å¤„ç†PILå›¾åƒ\n",
    "        img = input_image.convert('RGB')\n",
    "        return np.array(img).astype(imtype)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"å¼ é‡è½¬æ¢é”™è¯¯: {e}\")\n",
    "        # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return np.zeros((256, 256, 3), dtype=imtype)\n",
    "\n",
    "\n",
    "def preprocess_image(image, size=(256, 256)):\n",
    "    \"\"\"ç¡®ä¿æ‰€æœ‰è¾“å…¥å›¾åƒéƒ½æ˜¯RGBæ ¼å¼ï¼Œå½¢çŠ¶æ­£ç¡®\"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "        \n",
    "    # è½¬æ¢ä¸ºPILå›¾åƒ\n",
    "    if isinstance(image, np.ndarray):\n",
    "        print(f\"é¢„å¤„ç†å‰numpyæ•°ç»„å½¢çŠ¶: {image.shape}\")\n",
    "        # å¤„ç†å•é€šé“numpyæ•°ç»„\n",
    "        if image.ndim == 2 or (image.ndim == 3 and image.shape[-1] == 1):\n",
    "            image = Image.fromarray(image.squeeze(), mode='L').convert('RGB')\n",
    "        else:\n",
    "            image = Image.fromarray(image)\n",
    "    \n",
    "    # ç¡®ä¿æ˜¯RGBæ ¼å¼\n",
    "    if image.mode != 'RGB':\n",
    "        print(f\"è½¬æ¢å›¾åƒæ¨¡å¼ä» {image.mode} åˆ° RGB\")\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # è°ƒæ•´å¤§å°å¹¶å±…ä¸­è£å‰ª\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size, Image.BICUBIC),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # åº”ç”¨è½¬æ¢å¹¶è¿”å›\n",
    "    tensor = transform(image)\n",
    "    print(f\"é¢„å¤„ç†åå¼ é‡å½¢çŠ¶: {tensor.shape}\")\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "# 3. æ¨¡å‹åŠ è½½ï¼ˆæ”¹è¿›æƒé‡åŒ¹é…å’Œè°ƒè¯•ä¿¡æ¯ï¼‰\n",
    "def load_model(model_path):\n",
    "    \"\"\"ä¸¥æ ¼æŒ‰ç…§åŸå§‹pix2pixçš„U-Netç»“æ„åŠ è½½æ¨¡å‹\"\"\"\n",
    "    try:\n",
    "        # åˆ›å»ºæ¨¡å‹ï¼ˆä¸¥æ ¼åŒ¹é…åŸå§‹pix2pixæ¶æ„ï¼‰\n",
    "        model = UnetGenerator(\n",
    "            input_nc=3,\n",
    "            output_nc=3,\n",
    "            num_downs=8,\n",
    "            ngf=64,  # å…³é”®ï¼šä¸è®­ç»ƒæ—¶ngf=64åŒ¹é…\n",
    "            norm_layer=functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True),\n",
    "            use_dropout=False\n",
    "        )\n",
    "        \n",
    "        # æ‰“å°æ¨¡å‹ç»“æ„ç”¨äºè°ƒè¯•\n",
    "        print(\"æ¨¡å‹ç»“æ„:\")\n",
    "        print(model)\n",
    "        \n",
    "        # åŠ è½½æƒé‡\n",
    "        print(f\"ä» {model_path} åŠ è½½æ¨¡å‹æƒé‡...\")\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "        \n",
    "        # å°è¯•ä¸åŒçš„æƒé‡é”®å\n",
    "        possible_keys = ['generator_state_dict', 'netG', 'state_dict', 'model']\n",
    "        gen_weights = None\n",
    "        \n",
    "        for key in possible_keys:\n",
    "            if key in checkpoint:\n",
    "                gen_weights = checkpoint[key]\n",
    "                print(f\"ä»æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°æƒé‡é”®: {key}\")\n",
    "                break\n",
    "        \n",
    "        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šé”®ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ£€æŸ¥ç‚¹\n",
    "        if gen_weights is None:\n",
    "            gen_weights = checkpoint\n",
    "            print(\"ä½¿ç”¨æ•´ä¸ªæ£€æŸ¥ç‚¹ä½œä¸ºæƒé‡\")\n",
    "        \n",
    "        # æ¸…ç†æƒé‡é”®åï¼ˆç§»é™¤å¯èƒ½çš„å‰ç¼€ï¼‰\n",
    "        cleaned_weights = {}\n",
    "        prefixes = ['module.', 'netG.', 'model.']\n",
    "        \n",
    "        for k, v in gen_weights.items():\n",
    "            # å°è¯•ç§»é™¤å¸¸è§å‰ç¼€\n",
    "            cleaned_k = k\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_k.startswith(prefix):\n",
    "                    cleaned_k = cleaned_k[len(prefix):]\n",
    "                    break\n",
    "            cleaned_weights[cleaned_k] = v\n",
    "        \n",
    "        # éä¸¥æ ¼åŠ è½½ï¼ˆä»…åŒ¹é…å¯å…¼å®¹çš„å±‚ï¼‰\n",
    "        model_dict = model.state_dict()\n",
    "        matched_weights = {}\n",
    "        unmatched_weights = {}\n",
    "        \n",
    "        for k, v in cleaned_weights.items():\n",
    "            if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                matched_weights[k] = v\n",
    "            else:\n",
    "                unmatched_weights[k] = v\n",
    "        \n",
    "        print(f\"åŒ¹é…çš„æƒé‡: {len(matched_weights)}/{len(model_dict)}\")\n",
    "        print(f\"æœªåŒ¹é…çš„æƒé‡: {len(unmatched_weights)}\")\n",
    "        \n",
    "        # æ‰“å°æœªåŒ¹é…çš„æƒé‡ç”¨äºè°ƒè¯•\n",
    "        if len(unmatched_weights) > 0:\n",
    "            print(\"æœªåŒ¹é…çš„æƒé‡:\")\n",
    "            for k in unmatched_weights:\n",
    "                if k in model_dict:\n",
    "                    print(f\"  {k}: å½¢çŠ¶ä¸åŒ¹é… - æƒé‡å½¢çŠ¶ {unmatched_weights[k].shape}, æ¨¡å‹æœŸæœ› {model_dict[k].shape}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: æ¨¡å‹ä¸­ä¸å­˜åœ¨æ­¤é”®\")\n",
    "        \n",
    "        # æ›´æ–°æ¨¡å‹æƒé‡\n",
    "        model_dict.update(matched_weights)\n",
    "        model.load_state_dict(model_dict, strict=False)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"æˆåŠŸåŠ è½½æ¨¡å‹ï¼ˆåŒ¹é… {len(matched_weights)}/{len(model_dict)} å±‚ï¼‰\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½æ¨¡å‹å¤±è´¥: {e}\")\n",
    "        # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# 4. ç”Ÿæˆä¸ç•Œé¢é€»è¾‘\n",
    "def generate_image(input_image, model_path, dataset=None, dataset_index=None):\n",
    "    try:\n",
    "        print(\"\\n===== å¼€å§‹ç”Ÿæˆå›¾åƒ =====\")\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        model = load_model(model_path)\n",
    "        if model is None:\n",
    "            return None, None, None, \"æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆç»“æ„ä¸æƒé‡ä¸å…¼å®¹ï¼‰\"\n",
    "        \n",
    "        # å¤„ç†è¾“å…¥\n",
    "        if input_image is not None:\n",
    "            # æ‰“å°è¾“å…¥å›¾åƒä¿¡æ¯ç”¨äºè°ƒè¯•\n",
    "            if isinstance(input_image, np.ndarray):\n",
    "                print(f\"è¾“å…¥å›¾åƒç±»å‹: numpyæ•°ç»„, å½¢çŠ¶: {input_image.shape}, æ•°æ®ç±»å‹: {input_image.dtype}\")\n",
    "            elif isinstance(input_image, Image.Image):\n",
    "                print(f\"è¾“å…¥å›¾åƒç±»å‹: PILå›¾åƒ, æ¨¡å¼: {input_image.mode}, å¤§å°: {input_image.size}\")\n",
    "            \n",
    "            input_tensor = preprocess_image(input_image)\n",
    "            print(f\"è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}\")\n",
    "        else:\n",
    "            if dataset is None or dataset_index is None:\n",
    "                return None, None, None, \"è¯·ä¸Šä¼ å›¾åƒæˆ–é€‰æ‹©ç¤ºä¾‹\"\n",
    "            data = dataset[int(dataset_index)]\n",
    "            input_tensor = data['A'].unsqueeze(0)\n",
    "            print(f\"ä»æ•°æ®é›†ä¸­è·å–çš„è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}\")\n",
    "        \n",
    "        # ç”Ÿæˆå›¾åƒ\n",
    "        with torch.no_grad():\n",
    "            print(\"å¼€å§‹æ¨¡å‹æ¨ç†...\")\n",
    "            output_tensor = model(input_tensor.to(device))\n",
    "            print(f\"è¾“å‡ºå¼ é‡å½¢çŠ¶: {output_tensor.shape}\")\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå›¾åƒ\n",
    "        input_img = Image.fromarray(tensor2im(input_tensor))\n",
    "        gen_img = Image.fromarray(tensor2im(output_tensor))\n",
    "        \n",
    "        # è·å–çœŸå®å›¾åƒ\n",
    "        real_img = Image.new('RGB', (256, 256), color='gray')\n",
    "        if dataset and dataset_index is not None:\n",
    "            data = dataset[int(dataset_index)]\n",
    "            real_img = Image.fromarray(tensor2im(data['B']))\n",
    "        \n",
    "        print(\"===== å›¾åƒç”ŸæˆæˆåŠŸ =====\")\n",
    "        return input_img, gen_img, real_img, f\"ç”ŸæˆæˆåŠŸï¼ˆåŒ¹é… {len(model.state_dict())//2}/{len(model.state_dict())} å±‚ï¼‰\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {e}\")\n",
    "        # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, f\"é”™è¯¯: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_interface(model_path, dataset=None):\n",
    "    with gr.Blocks(title=\"è‰å›¾è½¬çœŸå®å›¾åƒ\") as demo:\n",
    "        gr.Markdown(\"# ğŸ¨ è‰å›¾è½¬å»ºç­‘å›¾åƒ\")\n",
    "        gr.Markdown(\"å·¦ï¼šè¾“å…¥è‰å›¾ | ä¸­ï¼šç”Ÿæˆå›¾åƒ | å³ï¼šçœŸå®å›¾åƒ\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                input_img = gr.Image(label=\"è¾“å…¥è‰å›¾\", type=\"pil\")\n",
    "                if dataset and len(dataset) > 0:\n",
    "                    idx_slider = gr.Slider(0, min(50, len(dataset)-1), 0, step=1, label=\"ç¤ºä¾‹ç´¢å¼•\")\n",
    "                else:\n",
    "                    idx_slider = None\n",
    "                gen_btn = gr.Button(\"ç”Ÿæˆå›¾åƒ\", variant=\"primary\")\n",
    "                status = gr.Textbox(label=\"çŠ¶æ€\", value=\"å°±ç»ª\")\n",
    "            \n",
    "            with gr.Column(scale=3):\n",
    "                with gr.Row():\n",
    "                    input_out = gr.Image(label=\"è¾“å…¥\")\n",
    "                    gen_out = gr.Image(label=\"ç”Ÿæˆ\")\n",
    "                    real_out = gr.Image(label=\"çœŸå®\")\n",
    "        \n",
    "        # ç»‘å®šäº‹ä»¶\n",
    "        if idx_slider is not None:\n",
    "            gen_btn.click(\n",
    "                fn=lambda img, idx: generate_image(img, model_path, dataset, idx),\n",
    "                inputs=[input_img, idx_slider],\n",
    "                outputs=[input_out, gen_out, real_out, status]\n",
    "            )\n",
    "        else:\n",
    "            gen_btn.click(\n",
    "                fn=lambda img: generate_image(img, model_path),\n",
    "                inputs=[input_img],\n",
    "                outputs=[input_out, gen_out, real_out, status]\n",
    "            )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "\n",
    "# 5. ä¸»å‡½æ•°\n",
    "if __name__ == \"__main__\":\n",
    "    class AlignedDataset:\n",
    "        def __init__(self, root, phase='val'):\n",
    "            self.root = os.path.join(root, phase)\n",
    "            self.images = [f for f in os.listdir(self.root) if f.endswith(('.png', '.jpg'))]\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img = Image.open(os.path.join(self.root, self.images[idx])).convert('RGB')\n",
    "            w, h = img.size\n",
    "            w2 = w // 2\n",
    "            return {\n",
    "                'A': transforms.ToTensor()(img.crop((w2, 0, w, h))),  # å³ä¾§è‰å›¾\n",
    "                'B': transforms.ToTensor()(img.crop((0, 0, w2, h)))   # å·¦ä¾§çœŸå®\n",
    "            }\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "    \n",
    "    model_path = \"checkpoints/pix2pix_epoch_100.pth\"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„\n",
    "    dataset = AlignedDataset(root=\"datasets/facades\", phase=\"val\")\n",
    "    \n",
    "    demo = create_interface(model_path, dataset)\n",
    "    port = find_available_port()\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
